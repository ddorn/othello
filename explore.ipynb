{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of OthelloGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch_lightning working\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mplotting\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m---> 27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mprobes\u001b[39;00m \u001b[39mimport\u001b[39;00m get_probe, get_neels_probe, ProbeTrainingArgs, LitLinearProbe\n\u001b[1;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mothello_world\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmechanistic_interpretability\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmech_interp_othello_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     plot_single_board, )\n\u001b[1;32m     32\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/ai/arena/othello/probes.py:186\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[39m@property\u001b[39m\n\u001b[1;32m    182\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mlength\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m    183\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_end \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_start\n\u001b[0;32m--> 186\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mLitLinearProbe\u001b[39;00m(pl\u001b[39m.\u001b[39mLightningModule):\n\u001b[1;32m    187\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"A linear probe for a transformer with its training configured for pytorch-lightning.\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    190\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    191\u001b[0m         model: HookedTransformer,\n\u001b[1;32m    192\u001b[0m         args: ProbeTrainingArgs,\n\u001b[1;32m    193\u001b[0m         \u001b[39m*\u001b[39mold_probes: Float[Tensor, \u001b[39m\"\u001b[39m\u001b[39md_model rows cols options\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    194\u001b[0m     ):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pl' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"ACCELERATE_DISABLE_RICH\"] = \"1\"\n",
    "# os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "from functools import partial\n",
    "from typing import Tuple, Union\n",
    "\n",
    "from circuitsvis.attention import attention_patterns\n",
    "import einops\n",
    "import torch as t\n",
    "import transformer_lens.utils as utils\n",
    "import wandb\n",
    "from jaxtyping import Float, Int\n",
    "from neel_plotly import line\n",
    "from torch import Tensor\n",
    "from transformer_lens import (\n",
    "    HookedTransformer,\n",
    ")\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "\n",
    "from plotly_utils import imshow\n",
    "\n",
    "from utils import *\n",
    "from plotting import *\n",
    "from probes import get_probe, get_neels_probe, ProbeTrainingArgs, LitLinearProbe\n",
    "\n",
    "from othello_world.mechanistic_interpretability.mech_interp_othello_utils import (\n",
    "    plot_single_board, )\n",
    "\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "    from pytorch_lightning.loggers import CSVLogger, WandbLogger\n",
    "except ValueError:\n",
    "    print(\"pytorch_lightning not working\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Things that you probably always want to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.set_grad_enabled(False)\n",
    "device = \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    "\n",
    "cfg, model = get_othello_gpt(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Loading sample data\n",
    "full_games_tokens, full_games_board_index = load_sample_games()\n",
    "num_games = 50\n",
    "focus_games_tokens = full_games_tokens[:num_games]\n",
    "focus_games_board_index = full_games_board_index[:num_games]\n",
    "assert (TOKENS_TO_BOARD[focus_games_tokens] == focus_games_board_index).all()\n",
    "\n",
    "focus_states = move_sequence_to_state(focus_games_board_index)\n",
    "focus_valid_moves = move_sequence_to_state(focus_games_board_index, mode=\"valid\")\n",
    "\n",
    "print(\"focus states:\", focus_states.shape)\n",
    "print(\"focus_valid_moves\", focus_valid_moves.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_heads = False\n",
    "\n",
    "n_games = 50\n",
    "tokens = full_games_tokens[-n_games:].to(device)\n",
    "board_index = full_games_board_index[-n_games:].to(device)\n",
    "get_metrics = lambda model: get_loss(model, tokens, board_index, 5, -5).to_tensor()\n",
    "zero_ablation_metrics = zero_ablation(model, get_metrics, individual_heads).cpu()\n",
    "base_metrics = get_metrics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "x_labels = [f\"Head {i}\" for i in range(model.cfg.n_heads)] + [\"All Heads\", \"MLP\"]\n",
    "y_labels = [f\"Layer {i}\" for i in range(model.cfg.n_layers)]\n",
    "if not individual_heads:\n",
    "    x_labels = x_labels[-2:]\n",
    "\n",
    "imshow(\n",
    "    zero_ablation_metrics[:3] - base_metrics[:3, None, None],\n",
    "    title=\"Metric increase after zeroing each component\",\n",
    "    x=x_labels,\n",
    "    y=y_labels,\n",
    "    facet_col=0,\n",
    "    facet_labels=[\"Loss\", \"Cell accuracy\", \"Board accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abblate all attention after the layer `n`\n",
    "\n",
    "def filter(name: str, start_layer: int = 0):\n",
    "    if not name.startswith(\"blocks.\"):\n",
    "        # 'hook_embed' or 'hook_pos_embed' or 'ln_final.hook_scale' or 'ln_final.hook_normalized'\n",
    "        return False\n",
    "\n",
    "    layer = int(name.split(\".\")[1])\n",
    "\n",
    "    return layer >= start_layer and \"attn_out\" in name\n",
    "\n",
    "metrics_per_layer = []\n",
    "for start_layer in range(model.cfg.n_layers):\n",
    "    with model.hooks(fwd_hooks=[(partial(filter, start_layer=start_layer),\n",
    "                                    zero_ablation_hook)]):\n",
    "        metrics_per_layer.append(get_metrics(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Plot\n",
    "lines = t.stack(metrics_per_layer, dim=1).cpu()\n",
    "line(\n",
    "    lines[:3],\n",
    "    x=[f\"â‰¥ {i}\" for i in range(model.cfg.n_layers)],\n",
    "    facet_col=0,\n",
    "    #  facet_col_wrap=3,\n",
    "    facet_labels=[\n",
    "        \"Loss\",\n",
    "        \"Cell accuracy\",\n",
    "        \"Board accuracy\",\n",
    "    ],  # 'False Positive', 'False Negative', 'True Positive', 'True Negative'],\n",
    "    title=\"Metrics after zeroing all attention heads above a layer\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of the probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration of the probe\n",
    "\n",
    "neel_probe = get_neels_probe(device)\n",
    "\"\"\"Shape (d_model, rows, cols, options)\n",
    "options: 0: blank, 1: my piece, 2: their piece\"\"\"\n",
    "\n",
    "blank_probe, my_probe, their_probe = neel_probe.unbind(dim=-1)\n",
    "blank_direction = blank_probe - (their_probe + my_probe) / 2\n",
    "my_direction = my_probe - their_probe\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_probe_accuracy(\n",
    "    model,\n",
    "    neel_probe,\n",
    "    focus_games_tokens,\n",
    "    focus_games_board_index,\n",
    "    # per_option=True,\n",
    "    per_move=\"board_accuracy\",\n",
    "    name=\"Neel's probe\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for probe, name in zip([blank_probe, their_probe, my_probe], [\"blank\", \"their\", \"my\"]):\n",
    "    probe = einops.rearrange(probe, \"d_model rows cols -> (rows cols) d_model\")\n",
    "    plot_similarities(probe,\n",
    "                      title=f\"Similarity between {name} vectors\",\n",
    "                      x=full_board_labels,\n",
    "                      y=full_board_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Similarity between mine and theirs (for each square)\n",
    "plot_similarities_2(my_probe, their_probe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import umap\n",
    "import umap.plot\n",
    "import pandas as pd\n",
    "\n",
    "vectors = einops.rearrange(linear_probe,\n",
    "                            \"d_model rows cols options -> (options rows cols) d_model\")\n",
    "\n",
    "mapper = umap.UMAP(metric=\"cosine\").fit(vectors.cpu().numpy())\n",
    "\n",
    "labels = [probe_name for probe_name in [\"blank\", \"their\", \"my\"] for _ in full_board_labels]\n",
    "hover_data = pd.DataFrame({\n",
    "    \"square\": full_board_labels * 3,\n",
    "    \"probe\": labels,\n",
    "})\n",
    "\n",
    "umap.plot.show_interactive(\n",
    "    umap.plot.interactive(mapper, labels=labels, hover_data=hover_data, theme=\"inferno\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_probe = get_probe(device)\n",
    "\n",
    "# %% Run PCA on the vectors of the probe\n",
    "vectors = einops.rearrange(linear_probe, \"d_model rows cols options -> (options rows cols) d_model\")\n",
    "plot_PCA(vectors, \"the probe vectors\")\n",
    "# %% The same be per option\n",
    "for i in range(3):\n",
    "    plot_PCA(\n",
    "        linear_probe[..., i],\n",
    "        f\"the probe vectors for option {i}\",\n",
    "        flip_dim_order=True,\n",
    "        absolute=True,\n",
    "    )\n",
    "\n",
    "# %% Normalise the probe then run PCA\n",
    "normalised_probe = linear_probe / linear_probe.norm(dim=-1, keepdim=True)\n",
    "plot_PCA(normalised_probe, \"the normalised probe vectors\", flip_dim_order=True)\n",
    "\n",
    "# %% Same PCA but with the unembeddings\n",
    "plot_PCA(model.W_U, \"the unembeddings\")\n",
    "\n",
    "# %%\n",
    "plot_PCA(model.W_pos, \"the embeddings\")\n",
    "# %%\n",
    "all_vectors = [\n",
    "    model.W_U.T,\n",
    "    model.W_E,\n",
    "    model.W_pos,\n",
    "    vectors,\n",
    "]\n",
    "all_vectors = [(v - v.mean(dim=0)) / v.std(dim=0) for v in all_vectors]\n",
    "\n",
    "plot_PCA(t.cat(all_vectors, dim=0), \"the embeddings and unembeddings\")\n",
    "# %%\n",
    "plot_PCA(t.cat([my_direction, blank_direction], dim=1).flatten(1).T, \"the direction vectors\")\n",
    "# %%\n",
    "plot_PCA(my_direction.flatten(1).T, \"the direction vectors\")\n",
    "# %%\n",
    "plot_PCA(blank_direction.flatten(1).T, \"the direction vectors\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "othello-wfvDuSXh-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
